{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Any\n",
    "\n",
    "class LabPredictor(ABC):\n",
    "    def __init__(self, model: Any=None) -> None:\n",
    "        self.model = model\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, text: str) -> List[str]:\n",
    "        \"\"\" the main predictor function. this should return a list of strings that will be visible in the frontend keyboard\n",
    "\n",
    "        Args:\n",
    "            text (str): the input text from the frontend keyboard\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self) -> None:\n",
    "        \"\"\" the main training function. this should train the model with the chosen data in each lab\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.text import TextCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk import *\n",
    "from nltk.corpus.reader.util import *\n",
    "import nltk.collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Welcome to the first lab!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pylint: disable=pointless-string-statement\n",
    "\"\"\" Welcome to the first lab!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel():\n",
    "    \"\"\" The main class for all n-gram models\n",
    "    Here you will create your model (based on N)\n",
    "    and complete the predict method to return the most likely words.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_gram=1) -> None:\n",
    "        \"\"\" the init method should load/train your model\n",
    "        Args:\n",
    "            n_gram (int, optional): 2=bigram, 2=trigram, ... Defaults to 1.\n",
    "        \"\"\"\n",
    "        print(f\"Loading {n_gram}-gram model...\")\n",
    "        self.n_gram = n_gram\n",
    "        self.words_to_return = 4  # how many words to show in the UI\n",
    "        self.model = BigramModel  # TODO: implement the model using built-in NLTK methods\n",
    "        # take a look at the nltk.collocations module\n",
    "        # https://www.nltk.org/howto/collocations.html\n",
    "    def predict(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\" given a list of tokens, return the most likely next words\n",
    "        Args:\n",
    "            tokens (List[str]): preprocessed tokens from the LabPredictor\n",
    "        Returns:\n",
    "            List[str]: selected candidates for next-word prediction\n",
    "        \"\"\"\n",
    "        # we're only interested in the last n-1 words.\n",
    "        # e.g. for a bigram model,\n",
    "        # we're only interested in the last word to predict the next\n",
    "        n_tokens = tokens[-(self.n_gram - 1):]\n",
    "        \n",
    "        probabilities = [] # TODO: find the probabilities for the next word(s)\n",
    "        \n",
    "            \n",
    "\n",
    "        # TODO: apply some filtering to only select the words\n",
    "        # here you're free to select your filtering methods\n",
    "        # a simple approach is to simply sort them by probability\n",
    "        best_matches = []  # TODO: sort/filter to your liking\n",
    "\n",
    "        # then return as many words as you've defined above\n",
    "        return best_matches[:self.words_to_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(NgramModel):\n",
    "    def __init__(self, corpustext) -> None:\n",
    "        super().__init__(n_gram=2)\n",
    "        self.corpustext = corpustext\n",
    "        print(self.corpustext + \" hey\")\n",
    "    \n",
    "    def collocations(self):\n",
    "        finder = BigramCollocationFinder.from_words(\n",
    "            self.words)\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures() # Measures unusual frequent bigram associations\n",
    "        finder.apply_freq_filter(7) # Add to get the most frequent expressions\n",
    "        finder.nbest(bigram_measures.pmi, 20) # Top collocations\n",
    "    \n",
    "    def find_tfidf(self):\n",
    "        vectorizer = CountVectorizer(ngram_range =(2, 2))\n",
    "        X1 = vectorizer.fit_transform(filtered)\n",
    "        features = (vectorizer.get_feature_names_out())\n",
    "        print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "        # Applying TFIDF\n",
    "        # You can still get n-grams here\n",
    "        vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "        X2 = vectorizer.fit_transform(filtered)\n",
    "        scores = (X2.toarray())\n",
    "        print(\"\\n\\nScores : \\n\", scores)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(NgramModel):\n",
    "    corpustext = None\n",
    "    def __init__(self, corpus: StreamBackedCorpusView) -> None:\n",
    "        super().__init__(n_gram=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lab1(LabPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.corpora = nltk.corpus.gutenberg  # TODO: load a corpus from NLTK\n",
    "\n",
    "        # Define a strategy to select the first words (when there's no input)\n",
    "        # TODO: this should not be a predefined list\n",
    "        self.start_words = [\"the\", \"a\", \"an\", \"this\", \"that\", \"these\", \"those\"]\n",
    "    @staticmethod\n",
    "    def preprocess(text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Preprocess the input text as you see fit, return a list of tokens.\n",
    "        - should you consider parentheses, punctuation?\n",
    "        - lowercase?\n",
    "        - find inspiration from the course literature :-)\n",
    "        \"\"\"\n",
    "        # TODO: filters here\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        filtered = []\n",
    "        for sentence in text:\n",
    "            for word in sentence:\n",
    "                if(word.isalpha() and word not in stopwords and len(word) > 2):\n",
    "                    filtered.append(word.lower())\n",
    "        return filtered # Tokenized and filtered\n",
    "    def predict(self, input_text):\n",
    "        if not bool(input_text):  # if there's no input...\n",
    "            print(\"No input, using start words\")\n",
    "            return self.start_words\n",
    "\n",
    "        # make use of the backoff model (e.g. bigram)\n",
    "        too_few = len(input_text) < 3  # TODO: check if the input is too short for trigrams\n",
    "        \n",
    "        tokens = self.preprocess(input_text)\n",
    "\n",
    "        # select the correct model based on the condition\n",
    "        model = self.backoff_model if too_few else self.model\n",
    "        # alternatively, you can switch between the tri- and bigram models\n",
    "        # based on the output probabilities. This is 100% optional!\n",
    "        return model.predict(tokens)\n",
    "    def train(self) -> None:\n",
    "        \"\"\" train or load the models\n",
    "        add parameters as you like, such as the corpora you selected.\n",
    "        \"\"\"\n",
    "        print(\"Training models...\")\n",
    "        self.model = TrigramModel(self.corpora.sents(\"chesterton-brown.txt\"))  # TODO: add needed parameters\n",
    "        print()\n",
    "        self.backoff_model = BigramModel(self.corpora.sents(\"chesterton-brown.txt\"))  # TODO: add needed parameters\n",
    "        self.sents = self.corpora.sents(\"chesterton-brown.txt\")\n",
    "        self.tokens = self.predict(self.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0ba2138ccdeef291c4f3939fc0ec3da14a9e1624c959932cf0ad354e60d5648"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
